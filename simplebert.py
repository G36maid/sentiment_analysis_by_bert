# -*- coding: utf-8 -*-
"""SimpleBert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MesKc2PIF9hqGWQ55iM1XtzRvQEu8wCc

In this file we try to fine-tune a simple model that can determin positive and negative comment from tweets
"""

#!pip install -q transformers
#!pip install -q datasets
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
from sklearn.model_selection import train_test_split



# Load the dataset
dataset = load_dataset("SetFit/tweet_sentiment_extraction")
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def splitData(df:pd.DataFrame, sz:float)->pd.DataFrame:
    x = df[df.columns[:2]]
    y = df[df.columns[2:4]]
    x_train, x_traindrop, y_train, y_traindrop = train_test_split(x, y, train_size=sz, random_state=seed)
    return pd.concat([x_train,y_train],axis=1)
def splitDataset(ds:DatasetDict, sz:float)->DatasetDict:
    train = pd.DataFrame(dataset['train'])
    test = pd.DataFrame(dataset['test'])
    train = splitData(train, sz)
    test = splitData(test, sz)
    return DatasetDict({'train':Dataset.from_pandas(train), 'test':Dataset.from_pandas(test)}) 


dataset = splitDataset(dataset, 0.5)

# Tokenization
def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

#since there are three labe: positive,negative,neutral the num_labels is 3
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

#!pip install accelerate -U
#!pip install transformers[torch]

# Training arguments with reduced batch size and gradient accumulation
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=6,  # Reduced batch size
    per_device_eval_batch_size=6,  # Reduced batch size
    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,  # Optional: Saves the best model based on evaluation
    save_strategy="epoch",  # Save at the end of each epoch to keep track of progress
)

# Trainer initialization remains the same
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

# Train the model
trainer.train()
try:
    model.save_pretrained('./saved_model')
    tokenizer.save_pretrained('./saved_model')
except:
    print('fail to save model')
# User text input for testing
user_input = input("Enter a tweet to analyze sentiment: ")
encoded_input = tokenizer(user_input, return_tensors='pt', padding=True, truncation=True)
output = model(**encoded_input)
probabilities = torch.nn.functional.softmax(output.logits, dim=-1)
predicted_sentiment = probabilities.argmax(-1).item()
probability_of_sentiment = probabilities[0, predicted_sentiment].item()
# Convert prediction to sentiment
sentiments = ['Negative', 'Neutral', 'Positive']  # Adjust as needed
print(f"Predicted Sentiment: {sentiments[predicted_sentiment]}, Probability: {probability_of_sentiment:.3f}")